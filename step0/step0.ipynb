{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0fQ33rWYUfv"
      },
      "source": [
        "# **Information Retrieval course (July 2023)**\n",
        "\n",
        "**Project Supervisor:** Prof. Alfio Ferrara\n",
        "\n",
        "**Provider:** Reza Ghahremani\n",
        "\n",
        "**Project Title:** Relation Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J1o4heOzB_Y"
      },
      "source": [
        "## Step 0: Requirements\n",
        "\n",
        "\n",
        "1.   Installing packages\n",
        "2.   Importing libraries\n",
        "3.   Defining functions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBS3rKGMzZRp"
      },
      "source": [
        "### Installing packages\n",
        "**1. Natural Language Toolkit:**\n",
        "\n",
        "**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for *classification, tokenization, stemming, tagging, parsing, and semantic reasoning*, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** https://www.nltk.org/ and\n",
        "https://www.nltk.org/install.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ9mDkiozh1T",
        "outputId": "6c6f864c-cd4b-407c-b75e-3269b4b62cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_alnGhP6d8xZ"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZs24DIP26-B"
      },
      "source": [
        "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
        "\n",
        "This tokenizer performs the following steps:\n",
        "\n",
        "\n",
        "*   split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
        "*   treat most punctuation characters as separate tokens\n",
        "*   split off commas and single quotes, when followed by whitespace\n",
        "*   separate periods that appear at the end of line\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1X9nWJqixNqo"
      },
      "outputs": [],
      "source": [
        "#Importing requirement libraries\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "os.environ['CLASSPATH'] = \"H:/Relation-Classification/stanford/stanford-postagger-2017-06-09\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jUORiuBTxhRZ"
      },
      "outputs": [],
      "source": [
        "#Reading files and creating new files\n",
        "\n",
        "train_file = './dataset/TRAIN_FILE.TXT'\n",
        "test_file = './dataset/TEST_FILE_FULL.TXT'\n",
        "\n",
        "new_train_file = \"./files/new_train_file.txt\"\n",
        "new_test_file = \"./files/new_test_file.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQjQOPa2nK5"
      },
      "source": [
        "### Difining Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkATIhlS22N7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "clean_tokens: this function is used to clean and normalize tokens by\n",
        "removing leading/trailing whitespace and converting multi-word tokens\n",
        "into a single string with underscores as separators.\n",
        "\"\"\"\n",
        "\n",
        "def clean_tokens(sentence_number, tokens):\n",
        "        temp = []\n",
        "        for tok in tokens:\n",
        "            tok = tok.strip().split()\n",
        "            if len(tok) > 1:\n",
        "                print(sentence_number, tok)\n",
        "            tok = \"_\".join(tok)\n",
        "            temp.append(tok)\n",
        "        return temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EpKhpAp-wZp7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "create_file: this function processes the input file line by line, transforming\n",
        "the sentences and their associated information into the desired format and\n",
        "writes the result to the output file.\n",
        "\"\"\"\n",
        "\n",
        "def create_file(filepath, outputpath):\n",
        "    file = open(outputpath, 'w')\n",
        "    lines = [line.strip() for line in open(filepath)]\n",
        "    for idx in range(0, len(lines), 4):\n",
        "        sentence_num = lines[idx].split(\"\\t\")[0]\n",
        "        sentence = lines[idx].split(\"\\t\")[1][1:-1]\n",
        "        label = lines[idx+1]\n",
        "\n",
        "        sentence = sentence.replace(\"<e1>\", \" E1_START \").replace(\"</e1>\", \" E1_END \")\n",
        "        sentence = sentence.replace(\"<e2>\", \" E2_START \").replace(\"</e2>\", \" E2_END \")\n",
        "\n",
        "        tokens = TreebankWordTokenizer().tokenize(sentence)\n",
        "        tokens = clean_tokens(sentence_num, tokens)\n",
        "\n",
        "        file.write(\" \".join([ label, \" \".join(tokens) ]))\n",
        "        file.write(\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    print(outputpath, \"created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBVk_GQFw4e8",
        "outputId": "d0daddaa-cc86-4f76-f78e-322b6cd0d094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./files/new_train_file.txt created\n",
            "./files/new_test_file.txt created\n",
            "Train / Test file created\n"
          ]
        }
      ],
      "source": [
        "create_file(train_file, new_train_file)\n",
        "create_file(test_file, new_test_file)\n",
        "\n",
        "print(\"Train / Test file created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L5pUYYpyZlwA"
      },
      "outputs": [],
      "source": [
        "new_train_file = './files/new_train_file.txt'\n",
        "new_test_file = './files/new_test_file.txt'\n",
        "\n",
        "new_train_file_with_line = './files/new_train_file_with_line.txt'\n",
        "val_file = './files/val_file.txt'\n",
        "new_test_file_with_line = './files/new_test_file_with_line.txt'\n",
        "\n",
        "train_answer_keys_path = './files/train_answer_keys.txt'\n",
        "val_answer_keys_path = './files/val_answer_keys.txt'\n",
        "test_answer_keys_path = './files/test_answer_keys.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWKPpIaTdMzj",
        "outputId": "6146a8b1-d807-4d0b-8704-a3e1ad27d927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./files/new_train_file.txt\n",
            "./files/new_train_file_with_line.txt\n",
            "./files/new_test_file.txt\n",
            "./files/new_test_file_with_line.txt\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "add_sent_number: this function takes an input file,\n",
        "adds line numbers to each line, and writes the modified\n",
        "lines to an output file.\n",
        "\"\"\"\n",
        "\n",
        "def add_sent_number(file_in, file_out):\n",
        "    print(file_in)\n",
        "    print(file_out)\n",
        "\n",
        "    f_in = open(file_in, 'r')\n",
        "    lines = f_in.readlines()\n",
        "    f_in.close()\n",
        "\n",
        "    f_out = open(file_out, 'w')\n",
        "    for i in range(len(lines)):\n",
        "        num = str(int(i+1))\n",
        "        ln = num + \" \" + lines[i]\n",
        "        f_out.write(ln)\n",
        "    f_out.close()\n",
        "\n",
        "# Call\n",
        "add_sent_number(new_train_file, new_train_file_with_line)\n",
        "add_sent_number(new_test_file, new_test_file_with_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yq1uCRrdN4r",
        "outputId": "62b79a2a-d5e8-4ec3-a99f-6bb324e32285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(val_index) 792\n",
            "val_index[:5] [9, 29, 31, 34, 35]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "get_val_sent_index:\n",
        "This function reads a file specified by new_train_file_with_line and extracts\n",
        "sentence numbers and labels from each line. It then randomly selects a\n",
        "subset of sentence numbers from each label and returns a sorted list of\n",
        "these selected numbers as the validation index.\n",
        "\"\"\"\n",
        "\n",
        "def get_val_sent_index():\n",
        "\n",
        "    global new_train_file_with_line\n",
        "\n",
        "    label_to_sent_num = {}\n",
        "\n",
        "    f_in = open(new_train_file_with_line, 'r')\n",
        "    lines = f_in.readlines()\n",
        "    f_in.close()\n",
        "\n",
        "    for l in lines:\n",
        "        l = l.strip().split(\" \")[:2]\n",
        "        num = int(l[0])\n",
        "        lab = str(l[1])\n",
        "\n",
        "        if lab not in label_to_sent_num:\n",
        "            label_to_sent_num[lab] = []\n",
        "\n",
        "        label_to_sent_num[lab].append(num)\n",
        "\n",
        "\n",
        "    val_index = []\n",
        "\n",
        "    for l in label_to_sent_num:\n",
        "        sent_num = label_to_sent_num[l]\n",
        "        num = int(len(sent_num) / 10)\n",
        "        random.shuffle(sent_num)\n",
        "        random.shuffle(sent_num)\n",
        "        val_index += sent_num[:num]\n",
        "\n",
        "    val_index = sorted(val_index)\n",
        "    print(\"len(val_index)\", len(val_index))\n",
        "    print(\"val_index[:5]\", val_index[:5])\n",
        "    return val_index\n",
        "\n",
        "\n",
        "# Call\n",
        "val_index = get_val_sent_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIoFlY4TdrSy",
        "outputId": "a24601d9-2608-4480-fc82-2b95959165bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Val - Split \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "train_val_split: this function reads the data from the new_train_file_with_line file,\n",
        "splits it into train and validation sets based on the val_index list, and writes\n",
        "the corresponding lines to the new_train_file_with_line and val_file files. It\n",
        "then prints a message indicating that the train and validation split has been performed.\n",
        "\"\"\"\n",
        "\n",
        "def train_val_split(val_index):\n",
        "    global new_train_file_with_line, val_file\n",
        "\n",
        "    f_in = open(new_train_file_with_line, 'r')\n",
        "    lines = f_in.readlines()\n",
        "    f_in.close()\n",
        "\n",
        "    f_train = open(new_train_file_with_line, 'w')\n",
        "    f_val = open(val_file, 'w')\n",
        "\n",
        "    for l in lines:\n",
        "        l = l.strip().split(\" \")\n",
        "        num = int(l[0])\n",
        "        lab = str(l[1])\n",
        "\n",
        "        if num in val_index:\n",
        "            f_val.write(\" \".join(l) + \"\\n\")\n",
        "        else:\n",
        "            f_train.write(\" \".join(l) + \"\\n\")\n",
        "\n",
        "    f_train.close()\n",
        "    f_val.close()\n",
        "\n",
        "    print(\"Train - Val - Split \")\n",
        "\n",
        "# Call\n",
        "train_val_split(val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCtbXtYwd9Dg",
        "outputId": "fe48157c-afbb-4ddd-b43b-d3dc191534ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./files/new_train_file_with_line.txt\n",
            "./files/val_file.txt\n",
            "8000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "train_val_total_check:\n",
        "This function calculates the total count of sentences by summing the counts\n",
        "from two files specified by new_train_file_with_line and val_file. It uses\n",
        "the get_count function internally to extract the counts of sentences for each\n",
        "label in each file.\n",
        "\"\"\"\n",
        "\n",
        "def train_val_total_check(train_attn_sp_path, val_attn_sp_path):\n",
        "\n",
        "    def get_count(file_path):\n",
        "        print(file_path)\n",
        "\n",
        "        label_to_sent_count = {}\n",
        "\n",
        "        f_in = open(file_path, 'r')\n",
        "        lines = f_in.readlines()\n",
        "        f_in.close()\n",
        "\n",
        "        for l in lines:\n",
        "            l = l.strip().split(\" \")[:2]\n",
        "            num = int(l[0])\n",
        "            lab = str(l[1])\n",
        "\n",
        "            if lab not in label_to_sent_count:\n",
        "                label_to_sent_count[lab] = 0\n",
        "\n",
        "            label_to_sent_count[lab] += 1\n",
        "\n",
        "        return label_to_sent_count\n",
        "\n",
        "    train = get_count(train_attn_sp_path)\n",
        "    val = get_count(val_attn_sp_path)\n",
        "\n",
        "    c = 0\n",
        "    for l in train:\n",
        "        c += train[l]\n",
        "        if l in val:\n",
        "            c += val[l]\n",
        "\n",
        "    print(c)\n",
        "\n",
        "\n",
        "# Call\n",
        "train_val_total_check(new_train_file_with_line, val_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QmyjQTZgd95o"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "create_answer_keys: this function reads a file specified by in_file,\n",
        "extracts line numbers and labels from each line, and writes them to a\n",
        "new file specified by out_file in the format of line number followed by\n",
        "a tab and the label. It then prints a message indicating that the output\n",
        "file has been created.\n",
        "\"\"\"\n",
        "\n",
        "def create_answer_keys(in_file, out_file):\n",
        "\n",
        "    f_in = open(in_file, 'r')\n",
        "    lines = f_in.readlines()\n",
        "    f_in.close()\n",
        "\n",
        "    f_out = open(out_file, 'w')\n",
        "\n",
        "    for i in range(0, len(lines)):\n",
        "        l = lines[i].strip().split(\" \")\n",
        "        num = str(i+1)\n",
        "        lab = str(l[1])\n",
        "        f_out.write(num + \"\\t\" + lab)\n",
        "        f_out.write(\"\\n\")\n",
        "    f_out.close()\n",
        "\n",
        "    print(out_file + \" \" + \"Created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk8c8eIdeFBK",
        "outputId": "a1d47e75-cb6f-40cc-9ffb-7c3c1385afc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./files/train_answer_keys.txt Created\n",
            "./files/val_answer_keys.txt Created\n",
            "./files/test_answer_keys.txt Created\n"
          ]
        }
      ],
      "source": [
        "create_answer_keys(new_train_file_with_line, train_answer_keys_path)\n",
        "create_answer_keys(val_file, val_answer_keys_path)\n",
        "create_answer_keys(new_test_file_with_line, test_answer_keys_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}